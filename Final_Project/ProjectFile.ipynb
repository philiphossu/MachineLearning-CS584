{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries, Importing Primary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phases 1-3 package imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing X dataset with actual image data\n",
    "\n",
    "X = pd.read_csv(\"X.csv\", sep=' ', header=None, dtype=float)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Bush, Williams label datasets\n",
    "\n",
    "y = pd.read_csv(\"y_bush_vs_others.csv\", header=None)\n",
    "y_bush = y.values.ravel()\n",
    "# Number of positive Bush instances: np.sum(y_bush) = 530\n",
    "y = pd.read_csv(\"y_williams_vs_others.csv\", header=None)\n",
    "y_williams = y.values.ravel() \n",
    "# Number of positive Williams instances: np.sum(y_williams) = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets contain 13233 items/labels, should return True\n",
    "\n",
    "X.shape[0] == y_bush.shape[0] == y_williams.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KNN and SVMs (Phase 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training and Testing KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating KNN classifiers which resulted in highest mean F1\n",
    "\n",
    "knn_bush = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_williams = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cross-validation to train using KNN\n",
    "\n",
    "cv_results_bush = cross_validate(knn_bush, X, y_bush, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                            scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "cv_results_bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results_williams = cross_validate(knn_williams, X, y_williams, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                            scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "cv_results_williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and Testing SVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SVC classifiers which resulted in highest mean F1\n",
    "\n",
    "svc_model_bush = SVC(C=10000, gamma=0.0001, kernel='rbf')\n",
    "svc_model_williams = SVC(C=0.04, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cross-validation to train using SVC\n",
    "\n",
    "cv_results_bush = cross_validate(svc_model_bush, X, y_bush, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                        scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "cv_results_bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_williams = cross_validate(svc_model_williams, X, y_williams, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                        scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "cv_results_williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying PCA for Dimensionality Reduction (Phase 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3072 = PCA(n_components=3072)\n",
    "pca_2048 = PCA(n_components=2048)\n",
    "pca_256 = PCA(n_components=256)\n",
    "pca_64 = PCA(n_components=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3072.fit(X)\n",
    "X_pca_3072 = pca_3072.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca_3072.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2048.fit(X)\n",
    "X_pca_2048 = pca_2048.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca_2048.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_256.fit(X)\n",
    "X_pca_256 = pca_256.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca_256.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_64.fit(X)\n",
    "X_pca_64 = pca_64.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca_64.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Applying PCA to KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_bush = cross_validate(knn_bush, X_pca_64, y_bush, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                            scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "print(cv_results_bush,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results_williams = cross_validate(knn_williams, X_pca_256, y_williams, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                            scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "print(cv_results_williams,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Applying PCA to SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3072 components on original Bush model\n",
    "cv_results_bush = cross_validate(svc_model_bush, X_pca_3072, y_bush, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                        scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "print(cv_results_bush,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2048 components on original Williams model\n",
    "cv_results_bush = cross_validate(svc_model_williams, X_pca_2048, y_williams, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                        scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "print(cv_results_bush,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New better Williams model\n",
    "svc_model_williams_2 = SVC(C=0.125, kernel='linear')\n",
    "\n",
    "# 2048 components on better Williams model\n",
    "cv_results_bush = cross_validate(svc_model_williams_2, X_pca_2048, y_williams, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=7000), \n",
    "                        scoring=('precision', 'recall', 'f1'), return_train_score=False, n_jobs=-1)\n",
    "print(cv_results_bush,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNNs (Phase 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Test/Train Split For Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bush\n",
    "X_train_bush, X_test_bush, y_train_bush, y_test_bush = train_test_split(\n",
    "    X, y_bush, test_size=1./3, random_state=4000, shuffle=True, stratify=y_bush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Williams\n",
    "X_train_williams, X_test_williams, y_train_williams, y_test_williams = train_test_split(\n",
    "    X, y_williams, test_size=1./3, random_state=4000, shuffle=True, stratify=y_williams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshaping Bush data points to 64x64 for use in CNN\n",
    "X_train_bush = X_train_bush.reshape(X_train_bush.shape[0],64,64,1) # last param: 1\n",
    "X_test_bush = X_test_bush.reshape(X_test_bush.shape[0],64,64,1) # last param: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Williams data points to 64x64 for use in CNN\n",
    "X_train_williams = X_train_williams.reshape(X_train_williams.shape[0],64,64,1) # last param: 1\n",
    "X_test_williams = X_test_williams.reshape(X_test_williams.shape[0],64,64,1) # last param: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing an image (Pete Sampras!)\n",
    "plt.imshow(X_test_bush[504].reshape(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing an image (George Bush!)\n",
    "plt.imshow(X_test_bush[517].reshape(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute F1\n",
    "def calc_f1(CNN_predicts, y_vals):\n",
    "    # Create confusion matrix\n",
    "    TP_b = 0; TN_b = 0; FP_b = 0; FN_b = 0\n",
    "    for i in range(0,len(CNN_predicts)):\n",
    "        if(y_vals[i] == 1 and CNN_predicts[i] == 1):\n",
    "            TP_b += 1\n",
    "        if(y_vals[i] == 0 and CNN_predicts[i] == 1):\n",
    "            FP_b += 1\n",
    "        if(y_vals[i] == 1 and CNN_predicts[i] == 0):\n",
    "            FN_b += 1\n",
    "        else:\n",
    "            TN_b += 1\n",
    "    \n",
    "    print(TP_b,TN_b,FP_b,FN_b)\n",
    "\n",
    "    b_accuracy = (TP_b+TN_b)/(TP_b+TN_b+FP_b+FN_b)\n",
    "    b_precision = (TP_b)/(TP_b+FP_b)\n",
    "    b_recall = (TP_b)/(TP_b+FN_b)\n",
    "    b_f1 = (2*b_precision*b_recall)/(b_precision+b_recall)\n",
    "    print(b_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating CNN Models Using Keras & Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bush CNN structure which resulted in optimal F1\n",
    "\n",
    "CNN_model_bush = Sequential()\n",
    "\n",
    "CNN_model_bush.add(Conv2D(128, kernel_size=2, activation=\"tanh\", input_shape=(64,64,1))) \n",
    "CNN_model_bush.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "CNN_model_bush.add(Conv2D(64, kernel_size=4, activation=\"tanh\")) \n",
    "CNN_model_bush.add(MaxPooling2D(pool_size=4)) \n",
    "\n",
    "CNN_model_bush.add(Flatten()) \n",
    "CNN_model_bush.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "CNN_model_bush.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Williams CNN structure which resulted in optimal F1\n",
    "\n",
    "CNN_model_williams = Sequential()\n",
    "\n",
    "CNN_model_williams.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(64,64,1)))\n",
    "CNN_model_williams.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "CNN_model_williams.add(Flatten())\n",
    "CNN_model_williams.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "CNN_model_williams.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training CNN Model To Bush, Williams Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bush\n",
    "CNN_model_bush.fit(X_train_bush, y_train_bush, validation_data=(X_test_bush, y_test_bush), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Williams\n",
    "CNN_model_williams.fit(X_train_williams, y_train_williams, validation_data=(X_test_williams, y_test_williams), epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Saved versions of these models are available in the saved folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculating F1 on Bush and Williams CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bush\n",
    "\n",
    "# On Test\n",
    "b_CNN_predicts_te = np.round(CNN_model_bush.predict(X_test_bush))\n",
    "print(np.sum(b_CNN_predicts)) # Number of predict true\n",
    "# On Train\n",
    "b_CNN_predict_tr = np.round(CNN_model_bush.predict(X_train_bush))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "calc_f1(b_CNN_predicts_te, y_test_bush)\n",
    "# On Train\n",
    "calc_f1(b_CNN_predict_tr, y_train_bush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Williams\n",
    "\n",
    "# On Test\n",
    "w_CNN_predicts_te = np.round(CNN_model_williams.predict(X_test_williams))\n",
    "np.sum(w_CNN_predicts) # Number of predict true\n",
    "# On Train\n",
    "w_CNN_predicts_tr = np.round(CNN_model_williams.predict(X_train_williams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "calc_f1(w_CNN_predicts_te, y_test_williams)\n",
    "# On Train\n",
    "calc_f1(w_CNN_predict_tr, y_train_williams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning (Phase 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Importing Dataset for Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time importing of Yale Extended B Faces Dataset\n",
    "\n",
    "X = []; y_18 = []; # y_22 = []\n",
    "i = 0\n",
    "folderslist = glob.glob(\"ExtendedYaleB/*/\")\n",
    "\n",
    "for i in range(0,len(folderslist)):\n",
    "    for filename in glob.glob(folderslist[i]+'*.pgm'):\n",
    "        # Open image\n",
    "        im = Image.open(filename)\n",
    "        # Crop image to square\n",
    "        width, height = im.size   \n",
    "        left = (width - 480)/2\n",
    "        top = (height - 480)/2\n",
    "        right = (width + 480)/2\n",
    "        bottom = (height + 480)/2\n",
    "        im = im.crop((left, top, right, bottom))\n",
    "        # Resize image to 64x64\n",
    "        im = im.resize((64,64), Image.ANTIALIAS)\n",
    "        # Convert to array for pickling purposes \n",
    "        im = np.array(im)\n",
    "        # Add to images list\n",
    "        X.append(im)\n",
    "        # If person 18, label in y_18\n",
    "        if(folderslist[i] == 'ExtendedYaleB/yaleB18/'):\n",
    "            y_18.append(1)\n",
    "        else:\n",
    "            y_18.append(0)\n",
    "    # Increment folders counter\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping data to an easier to load format\n",
    "pickle.dump([X,y_18], open(\"yale_data.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pickle Data \n",
    "tmp = pickle.load(open(\"yale_data.pickle\",\"rb\"), encoding='latin1')\n",
    "\n",
    "X = np.asarray(tmp[0])\n",
    "y_18 = np.asarray(tmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person 18 is the training target\n",
    "X_train_18, X_test_18, y_train_18, y_test_18 = train_test_split(\n",
    "    X, y_18, test_size=1./3, random_state=4000, shuffle=True, stratify=y_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Defining, Training, Testing on the Yale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "\n",
    "CNN_model_18 = Sequential()\n",
    "\n",
    "CNN_model_18.add(Conv2D(128, kernel_size=2, activation=\"tanh\", input_shape=(64, 64, 1))) \n",
    "CNN_model_18.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "CNN_model_18.add(Conv2D(64, kernel_size=4, activation=\"tanh\")) \n",
    "CNN_model_18.add(MaxPooling2D(pool_size=4))\n",
    "\n",
    "CNN_model_18.add(Conv2D(128, kernel_size=2, activation=\"tanh\")) \n",
    "CNN_model_18.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "CNN_model_18.add(Flatten()) \n",
    "\n",
    "CNN_model_18.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "CNN_model_18.fit(X_train_18, y_train_18, validation_data=(X_test_18, y_test_18), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "CNN_predicts = np.round(CNN_model_18.predict(X_test_18))\n",
    "# On Test\n",
    "calc_f1(CNN_predicts, y_test_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using Transfer Model on Bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading transfer model copy as \"transfer_mod_bush\"\n",
    "# transfer_mod_bush = copy.deepcopy(CNN_model_18)\n",
    "transfer_mod_bush.fit(X_train_bush, y_train_bush, validation_data=(X_test_bush, y_test_bush), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "b_CNN_predicts_te = np.round(transfer_mod_bush.predict(X_test_bush))\n",
    "# On Train\n",
    "b_CNN_predicts_tr = np.round(transfer_mod_bush.predict(X_train_bush))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "calc_f1(b_CNN_predicts_te, y_test_bush)\n",
    "# On Train\n",
    "calc_f1(b_CNN_predict_tr, y_train_bush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Using Transfer Model on Williams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading transfer model copy as \"transfer_mod_williams\"\n",
    "# transfer_mod_williams = copy.deepcopy(CNN_model_18)\n",
    "transfer_mod_williams.fit(X_train_williams, y_train_williams, validation_data=(X_test_williams, y_test_williams), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "w_CNN_predicts_te = np.round(transfer_mod_williams.predict(X_test_williams))\n",
    "# On Train\n",
    "w_CNN_predicts_tr = np.round(transfer_mod_williams.predict(X_train_williams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Test\n",
    "calc_f1(b_CNN_predicts_te, y_test_williams)\n",
    "# On Train\n",
    "calc_f1(b_CNN_predict_tr, y_train_williams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
